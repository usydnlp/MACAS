{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpN11Yu3IAqV"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn5hLt6aHq5l"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate, Activation\n",
    "from keras.utils import Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWxXvexCIKzI"
   },
   "source": [
    "### Change the number of epochs to 70 according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc4hRWn6L4G1"
   },
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "glove_pre = \"glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgRkrzgRIHoD"
   },
   "source": [
    "## Preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-4sYae1IMea"
   },
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename)\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append([splits[0],splits[-1]])\n",
    "\n",
    "    if len(sentence) >0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    return sentences\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "    \n",
    "\n",
    "def createBatches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches,batch_len\n",
    "\n",
    "def createBatches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches,batch_len\n",
    "\n",
    "def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def iterate_minibatches(dataset,batch_len): \n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        caseing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t,c,ch,l = dt\n",
    "            l = np.expand_dims(l,-1)\n",
    "            tokens.append(t)\n",
    "            caseing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n",
    "\n",
    "def addCharInformatioin(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            Sentences[i][j] = [data[0],chars,data[1]]\n",
    "    return Sentences\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apQ9q_epIiDJ"
   },
   "outputs": [],
   "source": [
    "trainSentences = readfile(\"train.txt\")\n",
    "devSentences = readfile(\"valid.txt\")\n",
    "testSentences = readfile(\"test.txt\")\n",
    "\n",
    "trainSentences = addCharInformatioin(trainSentences)\n",
    "devSentences = addCharInformatioin(devSentences)\n",
    "testSentences = addCharInformatioin(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVY4j2A5JzUl"
   },
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "words = {}\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True\n",
    "\n",
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "\n",
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')\n",
    "\n",
    "\n",
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "fEmbeddings = open(glove_pre, encoding=\"utf-8\")\n",
    "\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[split[0]] = len(word2Idx)\n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAzb523BKA8t"
   },
   "outputs": [],
   "source": [
    "train_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "dev_set = padding(createMatrices(devSentences,word2Idx, label2Idx, case2Idx,char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))\n",
    "\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "dev_batch,dev_batch_len = createBatches(dev_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gRTWsFOIarD"
   },
   "source": [
    "## Build and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "_iVaqVu3cEDM",
    "outputId": "d80a1e38-54b0-4ac4-c0be-489d42858056"
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words, casing,char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "# output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx)), name='before_softmax')(output)\n",
    "output = Activation('softmax')(output)\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "layer_name = 'before_softmax'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xDP2Tc85A-5"
   },
   "source": [
    "### Train the model \n",
    "Don't run these 3 sections, it will take about 1 hour to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TLE2_hPacFYv",
    "outputId": "f72986a9-004f-4827-8ed8-9c4b17d815d2"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, casing,char = batch       \n",
    "        model.train_on_batch([tokens, casing,char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "C4wh1LeasRsK",
    "outputId": "625eb9fb-252d-4cd3-d14c-015b7bf92aa3"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"NER.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"NER.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et6twnwn4TXp"
   },
   "source": [
    "### Load the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "G8IMcYhdBZQu",
    "outputId": "5ad959ab-2b9d-4f68-de56-e1960d56ab70"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('NER.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"NER.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XljbYw9g4X1c"
   },
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6371CUOtGY9y"
   },
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    before_softmaxs=[]\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]\n",
    "        before_softmax = intermediate_layer_model.predict([tokens, casing,char])[0]\n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        before_softmaxs.append(before_softmax)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    return predLabels, correctLabels, before_softmaxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STIr7BqlE3l3"
   },
   "outputs": [],
   "source": [
    "def get_NER_embedding(l):\n",
    "  new_list = []\n",
    "  for s in l:\n",
    "    to_add = []\n",
    "    for word in s:\n",
    "      to_add.append([word,\"O\\n\"])\n",
    "    new_list.append(to_add)\n",
    "  new_list = addCharInformatioin(new_list)\n",
    "  emb_set = padding(createMatrices(new_list,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "  emb_batch,_ = createBatches(emb_set)\n",
    "  _, _, before = tag_dataset(emb_batch)\n",
    "  return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "zjWifb6AIoPm",
    "outputId": "8070f8d4-dd4c-4c71-f4d1-57860f756c83"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ7ugpUvIqbx"
   },
   "outputs": [],
   "source": [
    "# change to your target dataset\n",
    "sent_text = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJ5RnfVTIsGT"
   },
   "outputs": [],
   "source": [
    "normalized_text=[]\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# Tokenising each sentence to process individual word\n",
    "sentences=[]\n",
    "sentences=[word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to your target dataset's max length\n",
    "MAX_LEN = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ugdxH_75Mwk"
   },
   "source": [
    "### Get the embeddings before softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_oJ8tT-SoCJ7",
    "outputId": "f1a755f7-c34b-436b-abc5-dfbde06479c9"
   },
   "outputs": [],
   "source": [
    "NER_embedding_list = []\n",
    "for sent in sentences:\n",
    "  this_array = get_NER_embedding([sent])[0].tolist()\n",
    "  if(len(sent)<MAX_LEN):\n",
    "    for i in range(MAX_LEN-len(sent)):\n",
    "      this_array.append([0]*9)\n",
    "  NER_embedding_list.append(this_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akusaa2ZoJkZ"
   },
   "outputs": [],
   "source": [
    "NER_embedding = np.array(NER_embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1UpVO7w4fQd"
   },
   "source": [
    "### Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5lJSkbOD30QF",
    "outputId": "6c91e4cd-b090-40db-dfcf-4e37ca6ebc51"
   },
   "outputs": [],
   "source": [
    "NER_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nep-QGEyoCAu"
   },
   "outputs": [],
   "source": [
    "np.save('NER_embedding.npy', NER_embedding)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "waseem_NER.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
