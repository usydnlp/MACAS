{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K65xH9lMb76F"
   },
   "source": [
    "## Dataset Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSF9iw6Zb-yA"
   },
   "outputs": [],
   "source": [
    "sentence_list =\n",
    "label_list = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIp6E71_cAHC"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "7FYcRwpRcDlC",
    "outputId": "3067a56b-5e83-41fc-a0c7-bff323e59805"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "tokenized_words = []\n",
    "for sentence in sentence_list:\n",
    "    clean_sent = re.sub(r'[^\\w\\s]','', sentence).lower()\n",
    "    tokenize_word = word_tokenize(clean_sent)\n",
    "    tokenized_words.append(tokenize_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ujhg-m27-598"
   },
   "source": [
    "#### seq_length (modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "i4lqWX7UcFHM",
    "outputId": "15630605-fa9e-4d34-e0a4-019fefc08d83"
   },
   "outputs": [],
   "source": [
    "length_list = [len(words) for words in tokenized_words]\n",
    "print(max(length_list))\n",
    "\n",
    "seq_length = \n",
    "# print(sum([1 for l in length_list if l<512])/len(length_list))\n",
    "# print(sum([1 for l in length_list if l<256])/len(length_list))\n",
    "# print(sum([1 for l in length_list if l<128])/len(length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "829wmv2JcoLN"
   },
   "source": [
    "### Word Index Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_DXKf25UcpUJ",
    "outputId": "1a903717-f7eb-4dc9-a9ea-766b27c9fe23"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenPAD = \"<pad>\"\n",
    "tokenUNK = \"<unknown>\"\n",
    "\n",
    "unique_words_temp = set()\n",
    "for sentence in tokenized_words:\n",
    "    for word in sentence:\n",
    "        unique_words_temp.add(word)\n",
    "unique_words = [tokenPAD, tokenUNK]\n",
    "unique_words.extend(unique_words_temp)\n",
    "\n",
    "unique_words_dict = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "\n",
    "word_index_encode = []\n",
    "for sentence in tokenized_words:\n",
    "    word_index_encode_temp = []    \n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_index_encode_temp.append(unique_words_dict[word])\n",
    "        except KeyError:\n",
    "            word_index_encode_temp.append(unique_words_dict[tokenUNK])\n",
    "    word_index_encode.append(word_index_encode_temp)\n",
    "\n",
    "#Pad sequences\n",
    "word_index_encode = pad_sequences(word_index_encode, maxlen = seq_length, value = unique_words_dict[tokenPAD], padding = \"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2aJn58Zdg0p"
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "INvgbTihdjau",
    "outputId": "315b7573-830c-4bb4-ca28-50c306c57882"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels=np.unique(label_list)\n",
    "\n",
    "num_class = len(labels)\n",
    "lEnc = LabelEncoder()\n",
    "lEnc.fit(labels)\n",
    "\n",
    "encoded_labels = lEnc.transform(label_list)\n",
    "\n",
    "print(labels)\n",
    "print(lEnc.transform(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7joyOxJePvU"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vodgaY8egNM"
   },
   "source": [
    "### Target Specific: NER (modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Fc_Oi9XJ__rH",
    "outputId": "ec5b3340-e901-4ebf-9e45-6951bb512633"
   },
   "outputs": [],
   "source": [
    "NER_embedding = np.load('NER_embedding.npy')\n",
    "print(NER_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc6xMqNpeWUs"
   },
   "source": [
    "### Target General: GP GN Golve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-IL_61DfzQB"
   },
   "outputs": [],
   "source": [
    "gpgn_glove_dict = pickle.load( open(\"gpgn_glove_dict.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c5DosXo1gmXD",
    "outputId": "4a22766a-efbe-4c1c-951b-ae8f7c56f44c"
   },
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((len(unique_words), 300))\n",
    "for i, word in enumerate(unique_words):\n",
    "    try:\n",
    "        if word != tokenPAD: #set pad weights to 0\n",
    "            glove_embedding_matrix[i] = gpgn_glove_dict[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mjgqtmCekMt"
   },
   "source": [
    "### Content Explicit: Dict2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asxasadhj_F_"
   },
   "outputs": [],
   "source": [
    "dict2vec_dict = pickle.load( open( \"dict2vec_dict.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WKNo_Npt1JPD",
    "outputId": "11293c1e-dd06-4a59-ff1f-e0f4b001fa9a"
   },
   "outputs": [],
   "source": [
    "dict2vec_embedding_matrix = np.zeros((len(unique_words), 300))\n",
    "for i, word in enumerate(unique_words):\n",
    "    try:\n",
    "        if word != tokenPAD: #set pad weights to 0\n",
    "            dict2vec_embedding_matrix[i] = dict2vec_dict[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "dict2vec_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP0l9KoweoVv"
   },
   "source": [
    "### Content Implicit: Sarcasm (modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "z-eJA6_9_9-z",
    "outputId": "24d6b2b7-b81b-4d55-c81e-8ad202e7ec2d"
   },
   "outputs": [],
   "source": [
    "sarcasm_embedding_list = []\n",
    "\n",
    "with open(\"sarcasm_embedding.txt\",\"r\") as f:\n",
    "  raw_string = f.readline()\n",
    "  raw_string = raw_string[1:-1]\n",
    "  \n",
    "  raw_lists = raw_string.split('[')\n",
    "  for l in raw_lists[1:]:\n",
    "    l = l[:-3]\n",
    "    float_list = l.split(\",\")\n",
    "    float_list = [float(f) for f in float_list]\n",
    "    sarcasm_embedding_list.append(float_list+[0]*7)\n",
    "sarcasm_embedding=np.array(sarcasm_embedding_list)   \n",
    "del sarcasm_embedding_list \n",
    "\n",
    "sarcasm_embedding=sarcasm_embedding.reshape((sarcasm_embedding.shape[0],1,-1))\n",
    "sarcasm_embedding=np.repeat(sarcasm_embedding,seq_length,1)\n",
    "print(sarcasm_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gQ_ce_xJPTr"
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "e1CegtKQJSxz",
    "outputId": "ec6ca0e3-fad5-4b8d-c376-2746dda7af9d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict2vec_dict = json.load( open( \"gcn_dict.json\", \"rb\" ) )\n",
    "\n",
    "gcn_embedding_matrix = np.zeros((len(unique_words), 200))\n",
    "for i, word in enumerate(unique_words):\n",
    "    try:\n",
    "        if word != tokenPAD: #set pad weights to 0\n",
    "            gcn_embedding_matrix[i] = dict2vec_dict[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "gcn_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsOmrp9_JJEF"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4eHFEQZJIO1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "index_train, index_test = train_test_split(word_index_encode, test_size=0.1, random_state=28)\n",
    "NER_train, NER_test = train_test_split(NER_embedding, test_size=0.1, random_state=28)\n",
    "sarcasm_train, sarcasm_test = train_test_split(sarcasm_embedding, test_size=0.1, random_state=28)\n",
    "label_train, label_test = train_test_split(encoded_labels, test_size=0.1, random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJGOCzkpPyd1"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "-EvuMQwAUlwo",
    "outputId": "eb25b309-fa6c-4c8c-f4e8-4d3e13b01b64"
   },
   "outputs": [],
   "source": [
    "!pip install keras-layer-normalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from datetime import datetime\n",
    "import keras.utils\n",
    "import keras.activations\n",
    "import keras.applications\n",
    "import keras.backend\n",
    "import keras.datasets\n",
    "import keras.engine\n",
    "import keras.layers\n",
    "import keras.preprocessing\n",
    "import keras.wrappers\n",
    "import keras.callbacks\n",
    "import keras.constraints\n",
    "import keras.initializers\n",
    "import keras.metrics\n",
    "import keras.models\n",
    "import keras.losses\n",
    "import keras.optimizers\n",
    "import keras.regularizers \n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RPsxO0YPz3J"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import math \n",
    "\n",
    "\n",
    "def matmul(x):\n",
    "    return tf.matmul(x[0], x[1])\n",
    "\n",
    "def split_heads(x,num_heads):\n",
    "    x = keras.layers.Reshape((seq_length, num_heads, -1))(x)\n",
    "    return keras.layers.Permute((2,1,3))(x)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, depth):\n",
    "  matmul_qk = keras.layers.Lambda(matmul)([q,keras.layers.Permute((1,3,2))(k)]) \n",
    "  # scale matmul_qk\n",
    "  scaled_attention_logits= keras.layers.Lambda(lambda x:x/math.sqrt(depth))(matmul_qk)\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = keras.layers.Activation('softmax')(scaled_attention_logits)  # (..., seq_len_q, seq_len_k)\n",
    "  output = keras.layers.Lambda(matmul)([attention_weights, v])\n",
    "  return output\n",
    "\n",
    "def new_trans(input_target_content,d_model,num_heads,dff,dropout):\n",
    "  input_target = input_target_content[0]\n",
    "  input_content = input_target_content[1]\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q_target_1 = keras.layers.Dense(d_model)(input_target)\n",
    "  k_target_1 = keras.layers.Dense(d_model)(input_target)\n",
    "  v_target_1 = keras.layers.Dense(d_model)(input_target)\n",
    "  q_content_1 = keras.layers.Dense(d_model)(input_content)\n",
    "  k_content_1 = keras.layers.Dense(d_model)(input_content)\n",
    "  v_content_1 = keras.layers.Dense(d_model)(input_content)\n",
    "\n",
    "  qw_target_1 = split_heads(q_target_1,num_heads)  \n",
    "  kw_target_1 = split_heads(k_target_1,num_heads)  \n",
    "  vw_target_1 = split_heads(v_target_1,num_heads) \n",
    "\n",
    "  qw_content_1 = split_heads(q_content_1,num_heads)  \n",
    "  kw_content_1 = split_heads(k_content_1,num_heads)  \n",
    "  vw_content_1 = split_heads(v_content_1,num_heads) \n",
    "\n",
    "  scaled_attention_content = scaled_dot_product_attention(qw_content_1, kw_target_1, vw_target_1, depth)\n",
    "  scaled_attention_content = keras.layers.Permute((2,1,3))(scaled_attention_content)  \n",
    "  concat_attention_content = keras.layers.Reshape((-1, d_model))(scaled_attention_content)\n",
    "  concat_attention_content = keras.layers.Dense(d_model)(concat_attention_content)\n",
    "  concat_attention_content = keras.layers.Dropout(dropout)(concat_attention_content)\n",
    "  out_1_content = LayerNormalization()(keras.layers.Add()([q_content_1,concat_attention_content]))\n",
    "  ffn_output_content = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_content))\n",
    "  ffn_output_content = keras.layers.Dropout(dropout)(ffn_output_content)\n",
    "  out_2_content = LayerNormalization()(keras.layers.Add()([ffn_output_content,out_1_content]))\n",
    "\n",
    "\n",
    "  scaled_attention_target = scaled_dot_product_attention(qw_target_1, kw_content_1, vw_content_1, depth)\n",
    "  scaled_attention_target = keras.layers.Permute((2,1,3))(scaled_attention_target)\n",
    "  concat_attention_target = keras.layers.Reshape((-1, d_model))(scaled_attention_target)\n",
    "  concat_attention_target = keras.layers.Dense(d_model)(concat_attention_target)\n",
    "  concat_attention_target = keras.layers.Dropout(dropout)(concat_attention_target)\n",
    "  out_1_target = LayerNormalization()(keras.layers.Add()([q_target_1,concat_attention_target]))\n",
    "  ffn_output_target = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_target))\n",
    "  ffn_output_target = keras.layers.Dropout(dropout)(ffn_output_target)\n",
    "  out_2_target = LayerNormalization()(keras.layers.Add()([ffn_output_target,out_1_target]))\n",
    "\n",
    "  return out_2_target,out_2_content\n",
    "\n",
    "def trans(input_emb, d_model,num_heads,dff,dropout):\n",
    "\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q = keras.layers.Dense(d_model)(input_emb)\n",
    "  k = keras.layers.Dense(d_model)(input_emb)\n",
    "  v = keras.layers.Dense(d_model)(input_emb)\n",
    "\n",
    "  qw = split_heads(q,num_heads)  \n",
    "  kw = split_heads(k,num_heads)  \n",
    "  vw = split_heads(v,num_heads) \n",
    "\n",
    "  scaled_attention = scaled_dot_product_attention(qw, kw, vw, depth)\n",
    "  scaled_attention = keras.layers.Permute((2,1,3))(scaled_attention)  \n",
    "  concat_attention = keras.layers.Reshape((-1, d_model))(scaled_attention)\n",
    "  concat_attention = keras.layers.Dense(d_model)(concat_attention)\n",
    "  concat_attention = keras.layers.Dropout(dropout)(concat_attention)\n",
    "  out_1 = LayerNormalization()(keras.layers.Add()([q,concat_attention]))\n",
    "  ffn_output = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1))\n",
    "  ffn_output = keras.layers.Dropout(dropout)(ffn_output)\n",
    "  out_2 = LayerNormalization()(keras.layers.Add()([ffn_output,out_1]))\n",
    "\n",
    "  return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4yhYSY4UxPq"
   },
   "outputs": [],
   "source": [
    "def get_model(use_gcn = 3):   \n",
    "\n",
    "    input_index = keras.layers.Input(shape = (index_train.shape[1],))\n",
    "    glove_emb =  keras.layers.Embedding(input_dim = glove_embedding_matrix.shape[0], output_dim = glove_embedding_matrix.shape[1], input_length = seq_length, weights = [glove_embedding_matrix], trainable = False)(input_index)\n",
    "    dict2vec_emb = keras.layers.Embedding(input_dim = dict2vec_embedding_matrix.shape[0], output_dim = dict2vec_embedding_matrix.shape[1], input_length = seq_length, weights = [dict2vec_embedding_matrix], trainable = False)(input_index)\n",
    "    gcn_emb = keras.layers.Embedding(input_dim = gcn_embedding_matrix.shape[0], output_dim = gcn_embedding_matrix.shape[1], input_length = seq_length, weights = [gcn_embedding_matrix], trainable = False)(input_index)\n",
    "    \n",
    "    input_NER = keras.layers.Input(shape=(seq_length,9))\n",
    "    input_sar = keras.layers.Input(shape=(seq_length,9))\n",
    "\n",
    "    input_target=keras.layers.Concatenate(axis=-1)([glove_emb,input_NER])\n",
    "    input_content=keras.layers.Concatenate(axis=-1)([dict2vec_emb,input_sar])\n",
    "\n",
    "    target_out, content_out = new_trans([input_target,input_content],d_model=309, num_heads=3, dff = 1296, dropout=0.5)\n",
    "    target_out_2, content_out_2 = new_trans([target_out,content_out],d_model=309, num_heads=3, dff = 1296, dropout=0.5)\n",
    "    encoded_output_tar = keras.layers.Dropout(0.1)(target_out_2)\n",
    "    encoded_output_lan = keras.layers.Dropout(0.1)(content_out_2)\n",
    " \n",
    "    gcn_encoder_out = trans(gcn_emb, d_model=200, num_heads=2, dff = 1296, dropout=0.5)\n",
    "    gcn_encoder_out_2 = trans(gcn_encoder_out, d_model=200, num_heads=2, dff = 1296, dropout=0.5)\n",
    "    encoded_output_gcn = keras.layers.Dropout(0.1)(gcn_encoder_out_2)\n",
    "\n",
    "    encoded_output_gcn_2 = keras.layers.AveragePooling1D(pool_size=3, strides=1)(encoded_output_gcn)\n",
    "    encoded_output_gcn_3 = keras.layers.AveragePooling1D(pool_size=3, strides=1)(encoded_output_gcn_2)\n",
    "    encoded_output_gcn_4 = keras.layers.AveragePooling1D(pool_size=3, strides=1)(encoded_output_gcn_3)\n",
    "\n",
    "    ######  lan-to-tar (target-gate) ######\n",
    "    hidden_size_lan = encoded_output_lan.shape[-1].value\n",
    "    sequence_length = encoded_output_lan.shape[-2].value\n",
    "    \n",
    "    #first\n",
    "    #target gate input preperation\n",
    "    flatten_tar_gate_input = keras.layers.Flatten()(encoded_output_lan)\n",
    "    log_probs_tar_gate_input = keras.layers.Dense(3, input_shape=(sequence_length, hidden_size_lan), activation='softmax')(flatten_tar_gate_input)\n",
    "    \n",
    "    lan_repeat_tar_gate = keras.layers.RepeatVector(sequence_length)(log_probs_tar_gate_input)\n",
    "    tar_gate_input_concat = keras.layers.Concatenate(axis=-1)([encoded_output_tar, lan_repeat_tar_gate])\n",
    "        \n",
    "    #gating \n",
    "    hidden_size_tar_gate_input_concat = tar_gate_input_concat.shape[-1].value\n",
    "\n",
    "    tar_concat_1 = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat, encoded_output_gcn])\n",
    "    tar_cnn_1 =  keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu')(tar_concat_1)\n",
    "\n",
    "    \n",
    "    ######  tar-to-lan (language-gate) ######\n",
    "    hidden_size_tar = encoded_output_tar.shape[-1].value\n",
    "    \n",
    "    #language gate input preperation \n",
    "    flatten_lan_gate_input = keras.layers.Flatten()(encoded_output_tar)\n",
    "    log_probs_lan_gate_input = keras.layers.Dense(3, input_shape=(sequence_length, hidden_size_tar), activation='softmax')(flatten_lan_gate_input)\n",
    "    \n",
    "    \n",
    "    tar_repeat_lan_gate = keras.layers.RepeatVector(sequence_length)(log_probs_lan_gate_input)\n",
    "    lan_gate_input_concat = keras.layers.Concatenate(axis=-1)([encoded_output_lan, tar_repeat_lan_gate])\n",
    "    \n",
    "    #gating\n",
    "    hidden_size_lan_gate_input_concat= lan_gate_input_concat.shape[-1].value\n",
    "\n",
    "    lan_concat_1 = keras.layers.Concatenate(axis=-1)([lan_gate_input_concat, encoded_output_gcn])\n",
    "    lan_cnn_1 =  keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu')(lan_concat_1)\n",
    "    \n",
    "    lan_cnn_1_size = lan_cnn_1.shape[-2].value\n",
    "    flatten_tar_cnn_1 = keras.layers.Flatten()(tar_cnn_1)\n",
    "    log_probs_tar_cnn_1 = keras.layers.Dense(3, activation='softmax')(flatten_tar_cnn_1)\n",
    "    tar_repeat_lan_gate_1 = keras.layers.RepeatVector(lan_cnn_1_size)(log_probs_tar_cnn_1)\n",
    "    tar_gate_input_concat_1 = keras.layers.Concatenate(axis=-1)([lan_cnn_1, tar_repeat_lan_gate_1])\n",
    "\n",
    "    flatten_lan_cnn_1 = keras.layers.Flatten()(lan_cnn_1)\n",
    "    log_probs_lan_cnn_1 = keras.layers.Dense(3, activation='softmax')(flatten_lan_cnn_1)\n",
    "    lan_repeat_tar_gate_1 = keras.layers.RepeatVector(lan_cnn_1_size)(log_probs_lan_cnn_1)\n",
    "    lan_gate_input_concat_1 = keras.layers.Concatenate(axis=-1)([tar_cnn_1, lan_repeat_tar_gate_1])\n",
    "\n",
    "    #second\n",
    "    lan_concat_1_size = lan_gate_input_concat_1.shape[-1].value\n",
    "    encoded_output_gcn_2 = keras.layers.Dense(lan_concat_1_size)(encoded_output_gcn_2)\n",
    "\n",
    "    tar_concat_2 = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat_1, encoded_output_gcn_2])\n",
    "    tar_cnn_2 =  keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu')(tar_concat_2)\n",
    "\n",
    "    lan_concat_2 = keras.layers.Concatenate(axis=-1)([lan_gate_input_concat_1, encoded_output_gcn_2])\n",
    "    lan_cnn_2 =  keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu')(lan_concat_2)\n",
    "\n",
    "    lan_cnn_2_size = lan_cnn_2.shape[-2].value\n",
    "    flatten_tar_cnn_2 = keras.layers.Flatten()(tar_cnn_2)\n",
    "    log_probs_tar_cnn_2 = keras.layers.Dense(3, activation='softmax')(flatten_tar_cnn_2)\n",
    "    tar_repeat_lan_gate_2 = keras.layers.RepeatVector(lan_cnn_2_size)(log_probs_tar_cnn_2)\n",
    "    tar_gate_input_concat_2 = keras.layers.Concatenate(axis=-1)([lan_cnn_2, tar_repeat_lan_gate_2])\n",
    "\n",
    "    flatten_lan_cnn_2 = keras.layers.Flatten()(lan_cnn_2)\n",
    "    log_probs_lan_cnn_2 = keras.layers.Dense(3, activation='softmax')(flatten_lan_cnn_2)\n",
    "    lan_repeat_tar_gate_2 = keras.layers.RepeatVector(lan_cnn_2_size)(log_probs_lan_cnn_2)\n",
    "    lan_gate_input_concat_2 = keras.layers.Concatenate(axis=-1)([tar_cnn_2, lan_repeat_tar_gate_2])\n",
    "\n",
    "    #third\n",
    "    lan_concat_2_size = lan_gate_input_concat_2.shape[-1].value\n",
    "    encoded_output_gcn_3 = keras.layers.Dense(lan_concat_2_size)(encoded_output_gcn_3)\n",
    "\n",
    "    tar_concat_3 = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat_2, encoded_output_gcn_3])\n",
    "    tar_cnn_3 =  keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(tar_concat_3)\n",
    "\n",
    "    lan_concat_3 = keras.layers.Concatenate(axis=-1)([lan_gate_input_concat_2, encoded_output_gcn_3])\n",
    "    lan_cnn_3 =  keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(lan_concat_3)\n",
    "\n",
    "    lan_cnn_3_size = lan_cnn_3.shape[-2].value\n",
    "    flatten_tar_cnn_3 = keras.layers.Flatten()(tar_cnn_3)\n",
    "    log_probs_tar_cnn_3 = keras.layers.Dense(3, activation='softmax')(flatten_tar_cnn_3)\n",
    "    tar_repeat_lan_gate_3 = keras.layers.RepeatVector(lan_cnn_3_size)(log_probs_tar_cnn_3)\n",
    "    tar_gate_input_concat_3 = keras.layers.Concatenate(axis=-1)([lan_cnn_3, tar_repeat_lan_gate_3])\n",
    "\n",
    "    flatten_lan_cnn_3 = keras.layers.Flatten()(lan_cnn_3)\n",
    "    log_probs_lan_cnn_3 = keras.layers.Dense(3, activation='softmax')(flatten_lan_cnn_3)\n",
    "    lan_repeat_tar_gate_3 = keras.layers.RepeatVector(lan_cnn_3_size)(log_probs_lan_cnn_3)\n",
    "    lan_gate_input_concat_3 = keras.layers.Concatenate(axis=-1)([tar_cnn_3, lan_repeat_tar_gate_3])\n",
    "\n",
    "\n",
    "    #concat\n",
    "    if use_gcn == 0:\n",
    "        concat_result = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat, lan_gate_input_concat])\n",
    "    elif use_gcn == 1:       \n",
    "        concat_result = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat_1, lan_gate_input_concat_1])\n",
    "    elif use_gcn == 2:\n",
    "        concat_result = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat_2, lan_gate_input_concat_2])\n",
    "    elif use_gcn == 3:\n",
    "        concat_result = keras.layers.Concatenate(axis=-1)([tar_gate_input_concat_3, lan_gate_input_concat_3])\n",
    "        \n",
    "    #classification\n",
    "    flatten = keras.layers.Flatten()(concat_result)\n",
    "    mlp_1 = keras.layers.Dense(512, activation=\"relu\")(flatten)\n",
    "    mlp_2 = keras.layers.Dense(64, activation=\"relu\")(mlp_1)\n",
    "    # mlp_3 = keras.layers.Dense(16, activation=\"relu\")(mlp_2)\n",
    "    dense_layer = keras.layers.Dense(2, activation=\"softmax\")(mlp_2)\n",
    "    \n",
    "    return keras.models.Model(inputs=[input_index, input_NER, input_sar], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv-olOxrDmQL"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def evaluate(trans_model, index_train, index_test, NER_train, NER_test, sarcasm_train, sarcasm_test, label_train, label_test, epochs, learningRate):\n",
    "  \n",
    "  trans_model.compile(optimizer= Adam(learningRate),loss='sparse_categorical_crossentropy',metrics = [\"accuracy\"])  \n",
    "  trans_model.summary()\n",
    "\n",
    "  current_time = datetime.now()\n",
    "  trans_model.fit([index_train, NER_train, sarcasm_train], label_train, batch_size=batch_size, epochs=epochs)\n",
    "  train_time=datetime.now() - current_time\n",
    "  print(\"Training took time: \", train_time)\n",
    "\n",
    "\n",
    "  #Predict classes\n",
    "  preds = trans_model.predict([index_test, NER_test, sarcasm_test], batch_size = batch_size)\n",
    "\n",
    "  return np.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "alMevWFxF9av",
    "outputId": "c5d1ddb4-8330-44e4-8803-1b27bad234f3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def plotConfusionMatrix(cmArray, labels, title, download=False):\n",
    "    df_cm = pd.DataFrame(cmArray, index = [i for i in labels],columns = [i for i in labels])\n",
    "  \n",
    "    plt.figure(figsize = (5,4))\n",
    "    #plt.title(title)\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "    if (download):\n",
    "        plt.savefig(title)\n",
    "        files.download(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pTLbkubOGDQU",
    "outputId": "78d7077a-3487-4f34-b84f-088dd9ed7827"
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs=7\n",
    "batch_size = 16\n",
    "learningRate=1e-6\n",
    "\n",
    "trans_model=get_model(use_gcn = 3)\n",
    "\n",
    "preds = evaluate(trans_model, index_train, index_test, NER_train, NER_test, sarcasm_train, sarcasm_test, label_train, label_test, epochs, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "mq9rGnEZXzMV",
    "outputId": "8f94ddd5-0b11-4b76-defe-240c6b67a612"
   },
   "outputs": [],
   "source": [
    "print(classification_report(lEnc.inverse_transform(label_test), lEnc.inverse_transform(preds),digits=4))\n",
    "\n",
    "cmArray = confusion_matrix(label_test, preds)\n",
    "print(pd.DataFrame(cmArray, columns = labels, index = labels))\n",
    "plotConfusionMatrix(cmArray, labels, \"20ng, 4e-5, epoch \"+str(epochs), False)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Abusive Final Test StormfrontWS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
