{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2W7wKTBfa71"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8kjn9U2iSDQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwUyGYQRhdzL"
   },
   "outputs": [],
   "source": [
    "original_train_sentences = \n",
    "original_labels_train = \n",
    "\n",
    "original_test_sentences = \n",
    "original_labels_test = \n",
    "\n",
    "original_sentences = original_train_sentences + original_test_sentences\n",
    "labels = original_labels_train + original_labels_test\n",
    "\n",
    "train_size = len(original_train_sentences)\n",
    "test_size = len(original_test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_sOZxaAqWpl"
   },
   "source": [
    "### Convert labels into one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq_cgRDpqaBz"
   },
   "outputs": [],
   "source": [
    "num_class = len(list(set(labels)))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lEnc = LabelEncoder()\n",
    "lEnc.fit(np.unique(labels))\n",
    "\n",
    "num_labels_train = lEnc.transform(original_labels_train)\n",
    "num_labels_test = lEnc.transform(original_labels_test)\n",
    "\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for l in num_labels_train:\n",
    "    to_add = [0]*num_class\n",
    "    to_add[l]=1\n",
    "    train_labels.append(to_add)\n",
    "\n",
    "for l in num_labels_test:\n",
    "    to_add = [0]*num_class\n",
    "    to_add[l]=1\n",
    "    test_labels.append(to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMkEBxr6fMQi"
   },
   "source": [
    "### Remove Stopwords and less frequent words, tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "1xRG94uDfaBV",
    "outputId": "fcdb69bc-3e14-47e8-d6b9-5aca894d4b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "{'who', 'how', 'have', 'being', \"mustn't\", 'whom', 'as', \"couldn't\", 'did', 'few', 'wasn', 'both', 'between', 're', 'their', 'an', 'she', \"she's\", 'that', \"needn't\", 'if', \"you've\", 'the', 'own', 'won', \"haven't\", 'too', 'same', \"weren't\", 'nor', 'her', 't', \"hadn't\", 'itself', 'do', 'each', 'to', 'my', 'd', 'been', 'hadn', 've', 'its', 'themselves', 'just', 'it', 'this', 'has', 'hasn', 'they', 'doing', 'before', 'll', 'having', 'until', 'there', 'myself', 'no', 'other', 'hers', 'off', \"that'll\", 'when', 'ma', 'aren', 'our', 'doesn', 'wouldn', \"won't\", 'was', 'mightn', 'of', 'his', 'can', 'once', 'only', 'o', 'up', 'with', \"hasn't\", 'on', 'in', 'from', 'where', \"you'll\", 'shan', 'very', 'after', 'theirs', 'out', 'shouldn', 'be', 'through', 'will', 'didn', 'we', \"wasn't\", 'here', \"aren't\", 'during', 'ourselves', 'further', 'ain', 'more', 'or', 'don', 'now', 'is', 'does', 'had', \"shouldn't\", 'down', 'then', 'at', 'while', 'you', 'most', 'what', 'against', 'some', 'isn', 'all', 'under', 'himself', 'but', 'm', \"didn't\", 'and', 'he', \"isn't\", 'for', 'your', 'which', 'yourselves', 'these', 'into', 'i', 'me', 'such', 'herself', 'so', 'am', 'by', 'again', 'were', \"should've\", \"don't\", 'why', 'should', \"shan't\", 'about', \"you'd\", 'them', 'not', 'those', 'because', 'yours', 'a', 'than', \"mightn't\", \"wouldn't\", \"you're\", 'him', 'couldn', 'mustn', 'yourself', 'are', \"it's\", 'y', 'needn', 'any', 'weren', 's', 'haven', 'below', \"doesn't\", 'above', 'ours', 'over'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFuDmPB_hLwO"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIccpgJJhR0R"
   },
   "outputs": [],
   "source": [
    "original_word_freq = {}  # to remove rare words\n",
    "for sentence in original_sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list = temp.split()\n",
    "    for word in word_list:\n",
    "        if word in original_word_freq:\n",
    "            original_word_freq[word] += 1\n",
    "        else:\n",
    "            original_word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H_3dWGOhkHo"
   },
   "outputs": [],
   "source": [
    "tokenize_sentences = []\n",
    "word_list_dict = {}\n",
    "for sentence in original_sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list_temp = temp.split()\n",
    "    doc_words = []\n",
    "    for word in word_list_temp:\n",
    "        if word not in stop_words and original_word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "            word_list_dict[word] = 1\n",
    "    tokenize_sentences.append(doc_words)\n",
    "word_list = list(word_list_dict.keys())\n",
    "vocab_length = len(word_list)\n",
    "\n",
    "del original_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzTc4kyG0ztR"
   },
   "outputs": [],
   "source": [
    "#word to id dict\n",
    "word_id_map = {}\n",
    "for i in range(vocab_length):\n",
    "    word_id_map[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znJ7Grz7fQ2L"
   },
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWSdrKICiXBa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QESQPT88AqsI"
   },
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wrB01uA4SWl"
   },
   "outputs": [],
   "source": [
    "window_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "pz_V-HSjQeQ3",
    "outputId": "0cfb6eb7-0dcb-418a-e9ba-28653b143a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3%\n",
      "7%\n",
      "11%\n",
      "15%\n",
      "19%\n",
      "23%\n",
      "27%\n",
      "31%\n",
      "35%\n",
      "39%\n",
      "43%\n",
      "47%\n",
      "51%\n",
      "55%\n",
      "59%\n",
      "63%\n",
      "67%\n",
      "71%\n",
      "75%\n",
      "79%\n",
      "83%\n",
      "87%\n",
      "91%\n",
      "95%\n",
      "99%\n"
     ]
    }
   ],
   "source": [
    "# word co-occurence with context windows\n",
    "import collections\n",
    "def get_dict(list_words):\n",
    "    out_dict ={}\n",
    "    unique_words = list(set(list_words))\n",
    "    for i in range(1, len(unique_words)):\n",
    "        word1 = unique_words[i]\n",
    "        for j in range(0,i):\n",
    "            word2 =  unique_words[j]\n",
    "            word_id1 = word_id_map[word1]\n",
    "            word_id2 = word_id_map[word2]\n",
    "            out_dict[str(word_id1)+','+str(word_id2)]=1\n",
    "            out_dict[str(word_id2)+','+str(word_id1)]=1\n",
    "    return out_dict\n",
    "\n",
    "windows=[]\n",
    "progress = 0.0\n",
    "sentence_list_of_dict=[]\n",
    "for words in tokenize_sentences:\n",
    "    list_of_dict = []\n",
    "    step = int(len(tokenize_sentences)/25)\n",
    "    progress+=1\n",
    "    if(progress%step==0):\n",
    "        print(str(int(progress/len(tokenize_sentences)*100))+\"%\")\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        unique_words = list(set(words))\n",
    "        windows.append(unique_words)\n",
    "        list_of_dict.append(get_dict(words))\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):           \n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(list(set(window)))\n",
    "            if(j==0):\n",
    "                last_round_dict = get_dict(window)\n",
    "            else:\n",
    "                this_round_dict = last_round_dict.copy()\n",
    "                new_word = window[-1]\n",
    "                delete_word = words[j-1]\n",
    "                if(new_word!=delete_word):\n",
    "                    if(new_word not in window[:-1]):\n",
    "                        for this_word in list(set(window[:-1])):\n",
    "                            word1_id = word_id_map[this_word]\n",
    "                            word2_id = word_id_map[new_word]\n",
    "                            this_round_dict[str(word1_id)+','+str(word2_id)]=1\n",
    "                            this_round_dict[str(word2_id)+','+str(word1_id)]=1\n",
    "                    if(delete_word not in window[:-1]):\n",
    "                        for this_word in list(set(window[:-1])):\n",
    "                            word1_id = word_id_map[this_word]\n",
    "                            word2_id = word_id_map[delete_word]\n",
    "                            del this_round_dict[str(word1_id)+','+str(word2_id)]\n",
    "                            del this_round_dict[str(word2_id)+','+str(word1_id)]\n",
    "                last_round_dict = this_round_dict.copy()\n",
    "            list_of_dict.append(last_round_dict)\n",
    "    counter = collections.Counter() \n",
    "    for d in list_of_dict:  \n",
    "        counter.update(d)       \n",
    "    word_pair_count = dict(counter) \n",
    "    sentence_list_of_dict.append(word_pair_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySBIcZGCSvWh"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter() \n",
    "for d in sentence_list_of_dict:  \n",
    "    counter.update(d) \n",
    "      \n",
    "word_pair_count = dict(counter) \n",
    "\n",
    "del sentence_list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-rxJKPY3bh-"
   },
   "outputs": [],
   "source": [
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmZaYh3T-Z7I"
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[word_list[i]]\n",
    "    word_freq_j = word_window_freq[word_list[j]]\n",
    "    pmi = log((count * num_window) /(word_freq_i * word_freq_j))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "del word_window_freq\n",
    "del word_pair_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynLnT3a33kW"
   },
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnSPqhg1lHps"
   },
   "outputs": [],
   "source": [
    "#get each word appears in which document\n",
    "word_doc_list = {}\n",
    "for word in word_list:\n",
    "    word_doc_list[word]=[]\n",
    "\n",
    "for i in range(len(tokenize_sentences)):\n",
    "    doc_words = tokenize_sentences[i]\n",
    "    unique_words = set(doc_words)\n",
    "    for word in unique_words:\n",
    "        exsit_list = word_doc_list[word]\n",
    "        exsit_list.append(i)\n",
    "        word_doc_list[word] = exsit_list\n",
    "\n",
    "#document frequency\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "# term frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[doc_id]\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6elPPFO_sXp"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[i]\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_length)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "del word_doc_freq\n",
    "del doc_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIkGgB2aZDk7"
   },
   "source": [
    "### Adjacent matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QKzgtt9_HzZ"
   },
   "outputs": [],
   "source": [
    "node_size = train_size + vocab_length + test_size\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu3pKndufr0-"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnpJ7hkp9Oa9"
   },
   "outputs": [],
   "source": [
    "# from inits import *\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]            \n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\t\t\t\n",
    "\t\t\n",
    "        self.embedding = output #output\n",
    "        return self.act(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUZKlFFV9MNM"
   },
   "outputs": [],
   "source": [
    "# from metrics import *\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "        self.pred = tf.argmax(self.outputs, 1)\n",
    "        self.labels = tf.argmax(self.placeholders['labels'], 1)\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            featureless=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "        \n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x, #\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Llor786EOY7C"
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    print(preds)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJA9yRNQIy-7"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# # Set random seed\n",
    "# seed = random.randint(1, 200)\n",
    "# np.random.seed(seed)\n",
    "# tf.set_random_seed(seed)\n",
    "\n",
    "\n",
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('dataset', dataset, 'Dataset string.')\n",
    "# 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
    "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0,\n",
    "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
    "flags.DEFINE_integer('early_stopping', 100,\n",
    "                     'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBANq_53r1qN"
   },
   "source": [
    "### Build symetric adjacent matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twKcK3tWLLdF"
   },
   "outputs": [],
   "source": [
    " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV02hmB1rLwm"
   },
   "source": [
    "### Construct features (for original GCN is identity matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HkcDz3cDLMb"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "features = sp.identity(adj.shape[0])\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyx_jR8lrH2f"
   },
   "source": [
    "### Construct y and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRdoGWSdHfRp"
   },
   "outputs": [],
   "source": [
    "real_train_size = int(0.9*train_size)\n",
    "val_size = train_size-real_train_size\n",
    "\n",
    "y_train=np.array([[0]*num_class]*adj.shape[0])\n",
    "y_val = np.array([[0]*num_class]*adj.shape[0])\n",
    "y_test=np.array([[0]*num_class]*adj.shape[0])\n",
    "\n",
    "y_train[:real_train_size] = train_labels[:real_train_size]\n",
    "y_val[real_train_size:train_size] = train_labels[real_train_size:]\n",
    "y_test[-test_size:] = test_labels\n",
    "\n",
    "train_mask = [False]*adj.shape[0]\n",
    "val_mask = [False]*adj.shape[0]\n",
    "test_mask = [False]*adj.shape[0]\n",
    "\n",
    "train_mask[:real_train_size] = [True]*real_train_size\n",
    "val_mask[real_train_size:train_size] = [True]*val_size\n",
    "test_mask[-test_size:] = [True]*test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ioXkX4UtSJ9"
   },
   "source": [
    "### Normalize adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMvShfgItRpZ"
   },
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "support = [preprocess_adj(adj)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHi74kUbELnk"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "GKHG-9nA9pba",
    "outputId": "c000dd78-979a-4c16-b2cc-a263354f7751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118685\n",
      "WARNING:tensorflow:From <ipython-input-25-bb0eb780351c>:123: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\", shape=(?, 4), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-27-25236610d06a>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_supports = 1\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    # helper variable for sparse dropout\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "print(features[2][1])\n",
    "model = GCN(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BXgM_ab-aRI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize session\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(\n",
    "        features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWrS6W6QOiXd"
   },
   "outputs": [],
   "source": [
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IdVNLOic_WY7",
    "outputId": "a27250b1-4638-4a43-ca4e-64b69b77c79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.38627 train_acc= 0.37127 val_loss= 1.37124 val_acc= 0.56989 time= 2.70464\n",
      "Epoch: 0002 train_loss= 1.37095 train_acc= 0.57296 val_loss= 1.33902 val_acc= 0.56689 time= 2.13453\n",
      "Epoch: 0003 train_loss= 1.33844 train_acc= 0.57057 val_loss= 1.29264 val_acc= 0.56700 time= 2.11171\n",
      "Epoch: 0004 train_loss= 1.29052 train_acc= 0.56953 val_loss= 1.23491 val_acc= 0.56833 time= 2.12746\n",
      "Epoch: 0005 train_loss= 1.23262 train_acc= 0.57199 val_loss= 1.17090 val_acc= 0.57155 time= 2.09992\n",
      "Epoch: 0006 train_loss= 1.16925 train_acc= 0.57391 val_loss= 1.10827 val_acc= 0.57522 time= 2.10584\n",
      "Epoch: 0007 train_loss= 1.10344 train_acc= 0.57807 val_loss= 1.05577 val_acc= 0.58000 time= 2.11155\n",
      "Epoch: 0008 train_loss= 1.05183 train_acc= 0.58164 val_loss= 1.01883 val_acc= 0.58455 time= 2.11115\n",
      "Epoch: 0009 train_loss= 1.01380 train_acc= 0.58674 val_loss= 0.99520 val_acc= 0.59211 time= 2.13291\n",
      "Epoch: 0010 train_loss= 0.98915 train_acc= 0.59563 val_loss= 0.97613 val_acc= 0.60389 time= 2.12459\n",
      "Epoch: 0011 train_loss= 0.96970 train_acc= 0.60921 val_loss= 0.95375 val_acc= 0.62577 time= 2.11087\n",
      "Epoch: 0012 train_loss= 0.94520 train_acc= 0.63222 val_loss= 0.92651 val_acc= 0.65333 time= 2.11414\n",
      "Epoch: 0013 train_loss= 0.91849 train_acc= 0.65679 val_loss= 0.89769 val_acc= 0.68000 time= 2.10599\n",
      "Epoch: 0014 train_loss= 0.89023 train_acc= 0.68206 val_loss= 0.87111 val_acc= 0.70344 time= 2.12570\n",
      "Epoch: 0015 train_loss= 0.85669 train_acc= 0.71079 val_loss= 0.84783 val_acc= 0.71589 time= 2.06249\n",
      "Epoch: 0016 train_loss= 0.83787 train_acc= 0.71873 val_loss= 0.82678 val_acc= 0.72055 time= 2.11826\n",
      "Epoch: 0017 train_loss= 0.81283 train_acc= 0.72651 val_loss= 0.80721 val_acc= 0.72155 time= 2.12207\n",
      "Epoch: 0018 train_loss= 0.79190 train_acc= 0.73035 val_loss= 0.79007 val_acc= 0.72322 time= 2.09884\n",
      "Epoch: 0019 train_loss= 0.77316 train_acc= 0.73320 val_loss= 0.77614 val_acc= 0.72489 time= 2.10946\n",
      "Epoch: 0020 train_loss= 0.75612 train_acc= 0.73553 val_loss= 0.76527 val_acc= 0.72633 time= 2.09141\n",
      "Epoch: 0021 train_loss= 0.74800 train_acc= 0.73507 val_loss= 0.75598 val_acc= 0.72977 time= 2.08753\n",
      "Epoch: 0022 train_loss= 0.73586 train_acc= 0.73757 val_loss= 0.74642 val_acc= 0.73011 time= 2.09565\n",
      "Epoch: 0023 train_loss= 0.72759 train_acc= 0.73578 val_loss= 0.73556 val_acc= 0.73155 time= 2.13255\n",
      "Epoch: 0024 train_loss= 0.70770 train_acc= 0.74261 val_loss= 0.72383 val_acc= 0.73411 time= 2.11438\n",
      "Epoch: 0025 train_loss= 0.70242 train_acc= 0.73983 val_loss= 0.71214 val_acc= 0.73866 time= 2.13790\n",
      "Epoch: 0026 train_loss= 0.67896 train_acc= 0.74914 val_loss= 0.70184 val_acc= 0.74111 time= 2.08876\n",
      "Epoch: 0027 train_loss= 0.67044 train_acc= 0.75093 val_loss= 0.69383 val_acc= 0.74589 time= 2.08782\n",
      "Epoch: 0028 train_loss= 0.65872 train_acc= 0.75530 val_loss= 0.68800 val_acc= 0.75000 time= 2.07749\n",
      "Epoch: 0029 train_loss= 0.64492 train_acc= 0.76141 val_loss= 0.68323 val_acc= 0.75211 time= 2.08166\n",
      "Epoch: 0030 train_loss= 0.63646 train_acc= 0.76302 val_loss= 0.67847 val_acc= 0.75444 time= 2.08987\n",
      "Epoch: 0031 train_loss= 0.62671 train_acc= 0.76792 val_loss= 0.67314 val_acc= 0.75711 time= 2.11406\n",
      "Epoch: 0032 train_loss= 0.61863 train_acc= 0.77073 val_loss= 0.66704 val_acc= 0.75811 time= 2.12451\n",
      "Epoch: 0033 train_loss= 0.61485 train_acc= 0.77150 val_loss= 0.66092 val_acc= 0.76022 time= 2.11247\n",
      "Epoch: 0034 train_loss= 0.60310 train_acc= 0.77548 val_loss= 0.65558 val_acc= 0.76089 time= 2.08084\n",
      "Epoch: 0035 train_loss= 0.58851 train_acc= 0.78283 val_loss= 0.65118 val_acc= 0.76322 time= 2.08229\n",
      "Epoch: 0036 train_loss= 0.58997 train_acc= 0.77996 val_loss= 0.64754 val_acc= 0.76344 time= 2.09019\n",
      "Epoch: 0037 train_loss= 0.58160 train_acc= 0.78290 val_loss= 0.64420 val_acc= 0.76489 time= 2.07801\n",
      "Epoch: 0038 train_loss= 0.57356 train_acc= 0.78602 val_loss= 0.64108 val_acc= 0.76866 time= 2.10538\n",
      "Epoch: 0039 train_loss= 0.56895 train_acc= 0.78722 val_loss= 0.63818 val_acc= 0.77089 time= 2.09640\n",
      "Epoch: 0040 train_loss= 0.56205 train_acc= 0.79050 val_loss= 0.63576 val_acc= 0.77211 time= 2.07377\n",
      "Epoch: 0041 train_loss= 0.55716 train_acc= 0.79098 val_loss= 0.63417 val_acc= 0.77144 time= 2.08194\n",
      "Epoch: 0042 train_loss= 0.54479 train_acc= 0.79671 val_loss= 0.63291 val_acc= 0.77089 time= 2.08588\n",
      "Epoch: 0043 train_loss= 0.54644 train_acc= 0.79490 val_loss= 0.63169 val_acc= 0.77255 time= 2.09174\n",
      "Epoch: 0044 train_loss= 0.54023 train_acc= 0.79640 val_loss= 0.63027 val_acc= 0.77244 time= 2.09255\n",
      "Epoch: 0045 train_loss= 0.53633 train_acc= 0.79660 val_loss= 0.62863 val_acc= 0.77311 time= 2.09165\n",
      "Epoch: 0046 train_loss= 0.52702 train_acc= 0.80150 val_loss= 0.62665 val_acc= 0.77311 time= 2.09893\n",
      "Epoch: 0047 train_loss= 0.52163 train_acc= 0.80312 val_loss= 0.62491 val_acc= 0.77266 time= 2.14420\n",
      "Epoch: 0048 train_loss= 0.51480 train_acc= 0.80621 val_loss= 0.62380 val_acc= 0.77522 time= 2.10206\n",
      "Epoch: 0049 train_loss= 0.51076 train_acc= 0.80686 val_loss= 0.62337 val_acc= 0.77489 time= 2.09486\n",
      "Epoch: 0050 train_loss= 0.50202 train_acc= 0.81234 val_loss= 0.62303 val_acc= 0.77444 time= 2.09982\n",
      "Epoch: 0051 train_loss= 0.50028 train_acc= 0.81302 val_loss= 0.62230 val_acc= 0.77622 time= 2.07778\n",
      "Epoch: 0052 train_loss= 0.49345 train_acc= 0.81695 val_loss= 0.62128 val_acc= 0.77911 time= 2.07660\n",
      "Epoch: 0053 train_loss= 0.48807 train_acc= 0.81793 val_loss= 0.62052 val_acc= 0.77900 time= 2.08232\n",
      "Epoch: 0054 train_loss= 0.48640 train_acc= 0.81733 val_loss= 0.62047 val_acc= 0.77733 time= 2.07950\n",
      "Epoch: 0055 train_loss= 0.47847 train_acc= 0.81996 val_loss= 0.62062 val_acc= 0.77611 time= 2.07150\n",
      "Epoch: 0056 train_loss= 0.47874 train_acc= 0.81821 val_loss= 0.62030 val_acc= 0.77711 time= 2.06697\n",
      "Epoch: 0057 train_loss= 0.47408 train_acc= 0.82050 val_loss= 0.62030 val_acc= 0.77833 time= 2.06660\n",
      "Epoch: 0058 train_loss= 0.46571 train_acc= 0.82505 val_loss= 0.62073 val_acc= 0.77733 time= 2.06642\n",
      "Epoch: 0059 train_loss= 0.46990 train_acc= 0.82408 val_loss= 0.62063 val_acc= 0.77733 time= 2.10476\n",
      "Epoch: 0060 train_loss= 0.46060 train_acc= 0.82953 val_loss= 0.61958 val_acc= 0.77755 time= 2.08222\n",
      "Epoch: 0061 train_loss= 0.45685 train_acc= 0.82958 val_loss= 0.62005 val_acc= 0.77566 time= 2.11317\n",
      "Epoch: 0062 train_loss= 0.45382 train_acc= 0.82964 val_loss= 0.62125 val_acc= 0.77544 time= 2.09891\n",
      "Epoch: 0063 train_loss= 0.44761 train_acc= 0.83292 val_loss= 0.62172 val_acc= 0.77511 time= 2.08015\n",
      "Epoch: 0064 train_loss= 0.44181 train_acc= 0.83452 val_loss= 0.62196 val_acc= 0.77533 time= 2.06483\n",
      "Epoch: 0065 train_loss= 0.44364 train_acc= 0.83424 val_loss= 0.62252 val_acc= 0.77644 time= 2.06656\n",
      "Epoch: 0066 train_loss= 0.43941 train_acc= 0.83678 val_loss= 0.62458 val_acc= 0.77678 time= 2.05840\n",
      "Epoch: 0067 train_loss= 0.43578 train_acc= 0.83729 val_loss= 0.62707 val_acc= 0.77733 time= 2.07396\n",
      "Epoch: 0068 train_loss= 0.43338 train_acc= 0.83996 val_loss= 0.62756 val_acc= 0.77666 time= 2.09102\n",
      "Epoch: 0069 train_loss= 0.42995 train_acc= 0.84090 val_loss= 0.62797 val_acc= 0.77500 time= 2.07359\n",
      "Epoch: 0070 train_loss= 0.43053 train_acc= 0.83731 val_loss= 0.62901 val_acc= 0.77522 time= 2.08150\n",
      "Epoch: 0071 train_loss= 0.41911 train_acc= 0.84292 val_loss= 0.63034 val_acc= 0.77500 time= 2.08376\n",
      "Epoch: 0072 train_loss= 0.41924 train_acc= 0.84294 val_loss= 0.63185 val_acc= 0.77322 time= 2.07033\n",
      "Epoch: 0073 train_loss= 0.42233 train_acc= 0.84113 val_loss= 0.63365 val_acc= 0.77389 time= 2.10405\n",
      "Epoch: 0074 train_loss= 0.41784 train_acc= 0.84323 val_loss= 0.63616 val_acc= 0.77511 time= 2.07369\n",
      "Epoch: 0075 train_loss= 0.41196 train_acc= 0.84713 val_loss= 0.63895 val_acc= 0.77633 time= 2.08132\n",
      "Epoch: 0076 train_loss= 0.40775 train_acc= 0.84955 val_loss= 0.64035 val_acc= 0.77578 time= 2.07608\n",
      "Epoch: 0077 train_loss= 0.40504 train_acc= 0.84966 val_loss= 0.64168 val_acc= 0.77333 time= 2.10120\n",
      "Epoch: 0078 train_loss= 0.40335 train_acc= 0.84763 val_loss= 0.64300 val_acc= 0.77433 time= 2.06884\n",
      "Epoch: 0079 train_loss= 0.40138 train_acc= 0.84791 val_loss= 0.64385 val_acc= 0.77289 time= 2.06663\n",
      "Epoch: 0080 train_loss= 0.40087 train_acc= 0.84844 val_loss= 0.64533 val_acc= 0.77178 time= 2.06781\n",
      "Epoch: 0081 train_loss= 0.40116 train_acc= 0.84996 val_loss= 0.64868 val_acc= 0.77044 time= 2.09009\n",
      "Epoch: 0082 train_loss= 0.39325 train_acc= 0.85386 val_loss= 0.65179 val_acc= 0.77211 time= 2.07127\n",
      "Epoch: 0083 train_loss= 0.39365 train_acc= 0.85521 val_loss= 0.65380 val_acc= 0.77222 time= 2.08028\n",
      "Epoch: 0084 train_loss= 0.38582 train_acc= 0.85631 val_loss= 0.65650 val_acc= 0.77166 time= 2.06822\n",
      "Epoch: 0085 train_loss= 0.38473 train_acc= 0.85646 val_loss= 0.65974 val_acc= 0.77055 time= 2.07899\n",
      "Epoch: 0086 train_loss= 0.39038 train_acc= 0.85084 val_loss= 0.66053 val_acc= 0.77066 time= 2.08172\n",
      "Epoch: 0087 train_loss= 0.38278 train_acc= 0.85631 val_loss= 0.66197 val_acc= 0.77078 time= 2.07576\n",
      "Epoch: 0088 train_loss= 0.37886 train_acc= 0.85942 val_loss= 0.66517 val_acc= 0.76833 time= 2.10563\n",
      "Epoch: 0089 train_loss= 0.38156 train_acc= 0.85850 val_loss= 0.66778 val_acc= 0.76811 time= 2.06498\n",
      "Epoch: 0090 train_loss= 0.37767 train_acc= 0.86131 val_loss= 0.66729 val_acc= 0.76800 time= 2.09636\n",
      "Epoch: 0091 train_loss= 0.37351 train_acc= 0.86043 val_loss= 0.67096 val_acc= 0.76600 time= 2.09062\n",
      "Epoch: 0092 train_loss= 0.37483 train_acc= 0.85758 val_loss= 0.67302 val_acc= 0.76533 time= 2.08052\n",
      "Epoch: 0093 train_loss= 0.37191 train_acc= 0.85942 val_loss= 0.67426 val_acc= 0.76611 time= 2.07565\n",
      "Epoch: 0094 train_loss= 0.36979 train_acc= 0.86173 val_loss= 0.67779 val_acc= 0.76433 time= 2.07718\n",
      "Epoch: 0095 train_loss= 0.36398 train_acc= 0.86690 val_loss= 0.68154 val_acc= 0.76244 time= 2.07180\n",
      "Epoch: 0096 train_loss= 0.36694 train_acc= 0.86528 val_loss= 0.68188 val_acc= 0.76422 time= 2.09135\n",
      "Epoch: 0097 train_loss= 0.36493 train_acc= 0.86592 val_loss= 0.68300 val_acc= 0.76489 time= 2.05575\n",
      "Epoch: 0098 train_loss= 0.36061 train_acc= 0.86527 val_loss= 0.68559 val_acc= 0.76366 time= 2.06718\n",
      "Epoch: 0099 train_loss= 0.35932 train_acc= 0.86472 val_loss= 0.68704 val_acc= 0.76489 time= 2.04517\n",
      "Epoch: 0100 train_loss= 0.35793 train_acc= 0.86465 val_loss= 0.68916 val_acc= 0.76344 time= 2.06236\n",
      "Epoch: 0101 train_loss= 0.35527 train_acc= 0.86846 val_loss= 0.69280 val_acc= 0.76144 time= 2.04949\n",
      "Epoch: 0102 train_loss= 0.35903 train_acc= 0.86810 val_loss= 0.69565 val_acc= 0.76100 time= 2.06906\n",
      "Epoch: 0103 train_loss= 0.35335 train_acc= 0.87080 val_loss= 0.69618 val_acc= 0.76266 time= 2.06642\n",
      "Epoch: 0104 train_loss= 0.34960 train_acc= 0.87083 val_loss= 0.69937 val_acc= 0.76166 time= 2.08096\n",
      "Epoch: 0105 train_loss= 0.34786 train_acc= 0.86949 val_loss= 0.70257 val_acc= 0.75978 time= 2.08129\n",
      "Early stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    \n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(\n",
    "        features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
    "                     model.layers[0].embedding, model.layers[1].embedding], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pred, labels, duration = evaluate(\n",
    "        features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(\n",
    "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zch8FwnvZWFf"
   },
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "f__MiWfG_ZAG",
    "outputId": "756309b0-6acf-4768-d96a-fc2930707882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.67268 accuracy= 0.76690 time= 0.72964\n",
      "118685\n",
      "Test Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8388    0.8256    0.8321      2735\n",
      "           1     0.4882    0.3120    0.3807       532\n",
      "           2     0.7946    0.8724    0.8317      5407\n",
      "           3     0.5116    0.3982    0.4478      1326\n",
      "\n",
      "    accuracy                         0.7669     10000\n",
      "   macro avg     0.6583    0.6021    0.6231     10000\n",
      "weighted avg     0.7529    0.7669    0.7569     10000\n",
      "\n",
      "Macro average Test Precision, Recall and F1-Score...\n",
      "(0.6583219083185988, 0.6020504789974914, 0.6231022711983202, None)\n",
      "Micro average Test Precision, Recall and F1-Score...\n",
      "(0.7669, 0.7669, 0.7669, None)\n",
      "embeddings:\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(\n",
    "    features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "print(len(test_mask))\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "\n",
    "# doc and word embeddings\n",
    "print('embeddings:')\n",
    "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = outs[3][:train_size]  # include val docs\n",
    "test_doc_embeddings = outs[3][adj.shape[0] - test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzLUnJKuU9NF"
   },
   "source": [
    "## Upload Word embeddings from original GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efJ3VI9wTFAY"
   },
   "outputs": [],
   "source": [
    "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\n",
    "\n",
    "word_emb_dict={}\n",
    "for i in range(len(word_list)):\n",
    "  word_emb_dict[word_list[i].strip()] = word_embeddings[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skJWuX5iWB-b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump( word_emb_dict, open( \"GCN_emb.json\", 'w' ) )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Founta _GCN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
